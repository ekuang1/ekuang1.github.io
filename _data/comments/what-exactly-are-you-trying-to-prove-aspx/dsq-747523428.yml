id: dsq-747523428
date: 2007-11-19T16:54:13.0000000-08:00
name: Frans Bouma
avatar: https://disqus.com/api/users/avatars/Frans Bouma.jpg
message: "<p>Critical parts, e.g. the entity management system internally, the save / load mechanism, the inheritance manager etc. indeed they're build with proved algorithms. Done on paper. It's not that hard really, you just have to use logic to prove the algorithm. <br>Another easy trick is to use proven algorithms which are proven by others. The world is full of algorithms already designed and proven for tasks used every day. Sedgewick's books are filled to the brim with them. <br>In logic, it's often easier to prove there's no possible way it will go wrong than it will always go right. <br>We do have bugs once in a while in our code. In far most of the time, they're in code where we didn't use algorithm proving on or we reviewed the code implementation not well enough so we overlooked some mistakes. As I've written in my Linq to LLBLGen Pro series before, we've been bitten by successful tests+sloppy algorithm usage ourselves a couple of years ago: the save train was broken, the algorithms used had flaws: in complex graphs things could go wrong. I then had enough of it and went back to the core and used a set of proven algorithms, and also re-checked the majority of the core code again: algorithms check, code review if the algorithm is implemented correctly, rinse repeat. We have a low bugrate due to that, I'm sure. <br>Without the algorithm proving and the reviewing of the code to see if we implemented the algorithm correctly, I would have no confidence if our code was OK to be shipped: I mean: on what ground can I conclude it's good enough otherwise? <br>We're all human, so we make mistakes. One can make a mistake in the proof of an algorithm, or overlook a mistake in the review of an implementation of it. That's also why we still use unit-tests for the integration testing (generated code + runtime lib + use cases =&gt; tests), but we don't rely on them for correctness, as said we have other ways to get correct code. I think it's essential in our line of business that we have a way to get correct code in our product, because others rely their software on our code, and also, unittests are pretty meaningless for a library with an infinite scope and an infinite number of input types/series/graphs etc. I mean: what do they tell you except you didn't break anything according to the tests (but you still might have broken something!)<br>That's IMHO unacceptable for a library which is a foundation of other software. It might take more time to write the code, but it does pay off. <br>Try it next time you have to write some code: what algorithm or algorithms are you using? Why these and not others? What makes them a perfect fit? Is the scope of the algorithm the same as the scope of the functionality? What are the states of the algorithm, is there a theoretical state possible that something went wrong? If so, how to get there? If it's not possible to reach that state, it's not possible to get into the state where things went wrong (e.g. quicksort which returns with an unsorted set). Etc. As an illustration of this, a mindgame:<br><a href=\"http://en.wikipedia.org/wiki/Two_Generals%27_Problem\" rel=\"nofollow noopener\" title=\"http://en.wikipedia.org/wiki/Two_Generals%27_Problem\">http://en.wikipedia.org/wik...</a><br>The logical proof that it's impossible to come up with a reliable algorithm/protocol to solve the problem illustrates how one can prove an algorithms correctness as well or when to expect which state. It's likely you already practise this kind of logic in your work in one way or the other.</p>"
